# RLFP Framework Development Plan

This document outlines the next steps for implementing the Reinforcement Learning from Preferences (RLFP) framework. The
goal is to create a system that can improve the agent's reasoning capabilities by learning from human feedback.

## 1. Flesh out the `ReasoningTrajectoryLogger`

The current logger is a skeleton. It needs to be integrated with the agent's event system to capture a detailed
reasoning trajectory.

- **Integrate with Agent's Event System:** Modify the `ReasoningTrajectoryLogger` to subscribe to the agent's event bus.
- **Capture Key Events:** The logger should capture not just the final output, but also intermediate steps. This
  includes:
    - The initial prompt sent to the Language Model (LLM).
    - Any tool calls the agent makes, including the tool name and arguments.
    - The results returned by the tools.
    - The final response generated by the agent.
- **Emit Events:** The core agent logic (specifically in `AgentStreamer.js`) will be modified to emit `LLM_PROMPT` and
  `TOOL_CALL` events with the relevant data payloads.

## 2. Implement a Basic `PreferenceCollector`

To gather feedback, we need a way to present pairs of trajectories to a user and record their preference.

- **File-Based System:** Since a full UI is out of scope, the implementation will be a simple, file-based system.
- **User Interaction:** The collector will be a script that can:
    - Load two trajectory files.
    - Display them side-by-side in the console.
    - Prompt the user to choose their preferred trajectory (e.g., by inputting 'A' or 'B').
    - Save the preference data (e.g., `(trajectory_A, trajectory_B, preference_A)`).

## 3. Implement a Proof-of-Concept `RLFPLearner`

The learner is the core of the RLFP loop, responsible for updating the model.

- **Research Fine-Tuning:** Investigate the capabilities of the `transformers.js` library for performing model updates
  or fine-tuning in a Node.js environment.
- **Simplified Update Mechanism:** The initial implementation will be a proof-of-concept. It will take a user preference
  and perform a simplified, conceptual update on the model. This will establish the API and data flow, even if the
  underlying learning algorithm is a placeholder.

## 4. Integrate and Test the Full RLFP Loop

The final step is to bring all the components together and demonstrate a complete feedback loop.

- **Create a New Example:** A new example script will be created to orchestrate the entire process.
- **Demonstrate the Loop:** The script will:
    1. Run the agent on a specific task to generate a reasoning trajectory.
    2. Run the agent on the same task with a slight variation (e.g., different prompt or temperature) to generate a
       second trajectory.
    3. Use the `ReasoningTrajectoryLogger` to save both trajectories.
    4. Use the `PreferenceCollector` to ask for user feedback on the two trajectories.
    5. Feed the resulting preference into the `RLFPLearner` to trigger a model update.
